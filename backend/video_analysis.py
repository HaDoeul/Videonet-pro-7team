"""
ë™ì˜ìƒ ë¶„ì„ ëª¨ë“ˆ - OpenCVì™€ GPT Vision API ì‚¬ìš©
ìŠ¬ë¼ì´ì‹± ê¸°ë°˜ ìš”ì•½ ë° ì¸ë¬¼ ì¸ì‹
"""

import cv2
import base64
import os
from pathlib import Path
from typing import List, Dict, Tuple, Any
import hashlib
from openai import OpenAI
from fastapi import APIRouter, UploadFile, File, HTTPException
from pydantic import BaseModel
import tempfile
import time

router = APIRouter(prefix="/api/video", tags=["video"])

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (lazy initialization)
# API í‚¤ê°€ ì—†ì–´ë„ ì„œë²„ê°€ ì‹œì‘ë˜ë„ë¡ í•¨
client = None

def get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° (í•„ìš”í•  ë•Œë§Œ ì´ˆê¸°í™”)"""
    global client
    if client is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise HTTPException(
                status_code=500,
                detail="OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”."
            )
        client = OpenAI(api_key=api_key)
    return client

class VideoAnalysisResult(BaseModel):
    """ë™ì˜ìƒ ë¶„ì„ ê²°ê³¼"""
    duration: float
    frame_count: int
    fps: float
    resolution: Tuple[int, int]
    file_size: int
    analysis_time: float
    summary: str
    persons_detected: List[Dict[str, Any]]
    key_frames: List[str]  # Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€

class FileVerificationResult(BaseModel):
    """íŒŒì¼ ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    original_hash: str
    received_hash: str
    file_size_match: bool
    original_size: int
    received_size: int
    verification_time: float

def calculate_file_hash(file_path: str) -> str:
    """íŒŒì¼ì˜ SHA256 í•´ì‹œ ê³„ì‚°"""
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()

def extract_key_frames(video_path: str, num_frames: int = 10) -> List[str]:
    """
    ë™ì˜ìƒì—ì„œ ì£¼ìš” í”„ë ˆì„ ì¶”ì¶œ (ìŠ¬ë¼ì´ì‹± ê¸°ë°˜)
    ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ìƒ˜í”Œë§
    """
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if total_frames == 0:
        cap.release()
        raise ValueError("ë™ì˜ìƒ ì½ê¸° ì‹¤íŒ¨")

    # ê· ë“±í•œ ê°„ê²©ìœ¼ë¡œ í”„ë ˆì„ ì„ íƒ
    frame_indices = [int(i * total_frames / num_frames) for i in range(num_frames)]

    key_frames = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()

        if ret:
            # JPEGë¡œ ì¸ì½”ë”© (ì••ì¶•ë¥  ë†’ì„)
            _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 60])
            # Base64 ì¸ì½”ë”©
            frame_b64 = base64.b64encode(buffer).decode('utf-8')
            key_frames.append(frame_b64)

    cap.release()
    return key_frames

def analyze_frame_with_gpt(frame_b64: str) -> Dict:
    """
    GPT Vision APIë¡œ í”„ë ˆì„ ë¶„ì„
    í† í° ì ˆì•½ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë³€í™˜
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        openai_client = get_openai_client()

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",  # ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì •ë³´ë¥¼ ê°„ë‹¨íˆ ì œê³µí•´ì£¼ì„¸ìš”:\n1. ì¸ë¬¼ ìˆ˜ (ëª‡ ëª…)\n2. ì£¼ìš” í™œë™/ì¥ë©´\n3. ë°°ê²½/ì¥ì†Œ\nìµœëŒ€í•œ ì§§ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{frame_b64}",
                                "detail": "low"  # ì €í•´ìƒë„ë¡œ ë¶„ì„ (í† í° ì ˆì•½)
                            }
                        }
                    ]
                }
            ],
            max_tokens=150
        )

        return {
            "description": response.choices[0].message.content,
            "tokens_used": response.usage.total_tokens
        }
    except Exception as e:
        print(f"GPT Vision ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            "description": "ë¶„ì„ ì‹¤íŒ¨",
            "tokens_used": 0
        }

@router.post("/analyze")
async def analyze_video(file: UploadFile = File(...)):
    """
    ë™ì˜ìƒ ë¶„ì„ API
    - ìŠ¬ë¼ì´ì‹± ê¸°ë°˜ ìš”ì•½
    - GPT Vision API ì¸ë¬¼ ì¸ì‹
    """
    start_time = time.time()

    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        tmp_path = tmp_file.name

    try:
        # ë™ì˜ìƒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        cap = cv2.VideoCapture(tmp_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = frame_count / fps if fps > 0 else 0
        cap.release()

        # íŒŒì¼ í¬ê¸°
        file_size = len(content)

        # ì£¼ìš” í”„ë ˆì„ ì¶”ì¶œ (10ê°œ)
        print("ğŸ“¸ ì£¼ìš” í”„ë ˆì„ ì¶”ì¶œ ì¤‘...")
        key_frames = extract_key_frames(tmp_path, num_frames=10)

        # GPT Visionìœ¼ë¡œ ì „ì²´ í”„ë ˆì„ ë¶„ì„
        print(f"ğŸ¤– GPT Vision ë¶„ì„ ì¤‘... (ì´ {len(key_frames)}ê°œ í”„ë ˆì„)")
        persons_detected = []
        total_tokens = 0
        has_person = False

        for i, frame_b64 in enumerate(key_frames):  # ì „ì²´ í”„ë ˆì„ ë¶„ì„
            print(f"  ğŸ“¸ í”„ë ˆì„ {i+1}/{len(key_frames)} ë¶„ì„ ì¤‘...")
            result = analyze_frame_with_gpt(frame_b64)

            # "ì¸ë¬¼ ì—†ìŒ" ê°ì§€
            description = result["description"]
            if "ì¸ë¬¼" in description.lower() and ("ì—†" in description or "0" in description or "ë¬´" in description):
                has_person_in_frame = False
            else:
                has_person_in_frame = True
                has_person = True

            persons_detected.append({
                "frame_index": i,
                "analysis": description,
                "has_person": has_person_in_frame,
                "tokens_used": result["tokens_used"]
            })
            total_tokens += result["tokens_used"]

        # ìš”ì•½ ìƒì„±
        summary = f"ë™ì˜ìƒ ê¸¸ì´: {duration:.2f}ì´ˆ, í•´ìƒë„: {width}x{height}, FPS: {fps:.2f}\n"
        summary += f"ì „ì²´ í”„ë ˆì„ ìˆ˜: {frame_count}ê°œ, ë¶„ì„ëœ í”„ë ˆì„ ìˆ˜: {len(key_frames)}ê°œ\n"
        summary += f"ì´ ì‚¬ìš© í† í°: {total_tokens}ê°œ\n\n"

        if not has_person:
            summary += "âš ï¸ ë™ì˜ìƒ ì „ì²´ì—ì„œ ì¸ë¬¼ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            summary += "âœ… ì¸ë¬¼ì´ ê°ì§€ëœ í”„ë ˆì„:\n"
            for p in persons_detected:
                if p['has_person']:
                    summary += f"  - í”„ë ˆì„ {p['frame_index']+1}: {p['analysis']}\n"

            summary += "\nâŒ ì¸ë¬¼ì´ ì—†ëŠ” í”„ë ˆì„:\n"
            for p in persons_detected:
                if not p['has_person']:
                    summary += f"  - í”„ë ˆì„ {p['frame_index']+1}\n"

        analysis_time = time.time() - start_time

        print(f"âœ… ë¶„ì„ ì™„ë£Œ (ì´ {total_tokens} í† í° ì‚¬ìš©, {analysis_time:.2f}ì´ˆ)")

        return VideoAnalysisResult(
            duration=duration,
            frame_count=frame_count,
            fps=fps,
            resolution=(width, height),
            file_size=file_size,
            analysis_time=analysis_time,
            summary=summary,
            persons_detected=persons_detected,
            key_frames=key_frames  # ì „ì²´ í”„ë ˆì„ ë°˜í™˜
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë™ì˜ìƒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

@router.post("/verify")
async def verify_file(original_file: UploadFile = File(...), received_file: UploadFile = File(...)):
    """
    íŒŒì¼ ê²€ì¦ API
    - SHA256 í•´ì‹œ ë¹„êµ
    - íŒŒì¼ í¬ê¸° ë¹„êµ
    """
    start_time = time.time()

    # ì›ë³¸ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False) as tmp1:
        original_content = await original_file.read()
        tmp1.write(original_content)
        tmp1_path = tmp1.name

    # ìˆ˜ì‹  íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False) as tmp2:
        received_content = await received_file.read()
        tmp2.write(received_content)
        tmp2_path = tmp2.name

    try:
        # í•´ì‹œ ê³„ì‚°
        print("ğŸ” íŒŒì¼ í•´ì‹œ ê³„ì‚° ì¤‘...")
        original_hash = calculate_file_hash(tmp1_path)
        received_hash = calculate_file_hash(tmp2_path)

        # í¬ê¸° ë¹„êµ
        original_size = len(original_content)
        received_size = len(received_content)

        is_valid = (original_hash == received_hash) and (original_size == received_size)
        verification_time = time.time() - start_time

        print(f"{'âœ… ê²€ì¦ ì„±ê³µ' if is_valid else 'âŒ ê²€ì¦ ì‹¤íŒ¨'} ({verification_time:.2f}ì´ˆ)")

        return FileVerificationResult(
            is_valid=is_valid,
            original_hash=original_hash,
            received_hash=received_hash,
            file_size_match=(original_size == received_size),
            original_size=original_size,
            received_size=received_size,
            verification_time=verification_time
        )

    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists(tmp1_path):
            os.remove(tmp1_path)
        if os.path.exists(tmp2_path):
            os.remove(tmp2_path)

# ===== ì±„íŒ… ì„¸ì…˜ ê´€ë¦¬ =====

# ë©”ëª¨ë¦¬ì— ì±„íŒ… ì„¸ì…˜ ì €ì¥ (íŒŒì¼ëª…ì„ í‚¤ë¡œ ì‚¬ìš©)
chat_sessions: Dict[str, List[Dict[str, str]]] = {}

class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""
    question: str
    analysisResult: Dict[str, Any]
    videoInfo: Dict[str, Any]
    chatHistory: List[Dict[str, str]] = []  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ì±„íŒ… ê¸°ë¡

@router.post("/chat")
async def chat_with_analysis(request: ChatRequest):
    """
    ë™ì˜ìƒ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì±„íŒ… API
    - ë¶„ì„ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
    - GPTë¥¼ í†µí•´ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€
    - ëŒ€í™” ê¸°ì–µë ¥ ë³´ì¥ (ì „ì²´ ì±„íŒ… íˆìŠ¤í† ë¦¬ í¬í•¨)
    """
    try:
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
        openai_client = get_openai_client()

        filename = request.videoInfo.get('filename', 'unknown')

        # ì„¸ì…˜ ì´ˆê¸°í™” (íŒŒì¼ë³„ë¡œ ëŒ€í™” ì €ì¥)
        if filename not in chat_sessions:
            chat_sessions[filename] = []

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë™ì˜ìƒ ì •ë³´)
        context = f"""
ë™ì˜ìƒ ì •ë³´:
- íŒŒì¼ëª…: {filename}
- ê¸¸ì´: {request.videoInfo.get('duration', 0):.2f}ì´ˆ
- í•´ìƒë„: {request.videoInfo.get('resolution', [0, 0])[0]}x{request.videoInfo.get('resolution', [0, 0])[1]}

ë¶„ì„ ìš”ì•½:
{request.analysisResult.get('summary', '')}

ì¸ë¬¼ ê°ì§€ ì •ë³´:
"""
        # ì¸ë¬¼ ì •ë³´ ì¶”ê°€
        for person in request.analysisResult.get('persons_detected', []):
            context += f"\n- í”„ë ˆì„ {person.get('frame_index', 0) + 1}: {person.get('analysis', '')}"

        # GPT ë©”ì‹œì§€ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì»¨í…ìŠ¤íŠ¸ + ì „ì²´ ëŒ€í™” ê¸°ë¡)
        messages = [
            {
                "role": "system",
                "content": f"""ë‹¹ì‹ ì€ ë™ì˜ìƒ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë™ì˜ìƒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë™ì˜ìƒì— ë“±ì¥í•˜ëŠ” ì¸ë¬¼ì˜ íŠ¹ì§•, ì¥ë©´ ì„¤ëª…, ë™ì˜ìƒ ìš”ì•½ ë“±ì„ ëª…í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
ë¶„ì„ ê²°ê³¼ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , "ë¶„ì„ ê²°ê³¼ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”.

{context}"""
            }
        ]

        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ ì±„íŒ… íˆìŠ¤í† ë¦¬ ì¶”ê°€ (assistantì˜ ì´ˆê¸° ë©”ì‹œì§€ ì œì™¸)
        for msg in request.chatHistory:
            if msg['role'] == 'user' or (msg['role'] == 'assistant' and 'ë™ì˜ìƒ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤' not in msg['content']):
                messages.append({
                    "role": msg['role'],
                    "content": msg['content']
                })

        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        messages.append({
            "role": "user",
            "content": request.question
        })

        # GPT í˜¸ì¶œ
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=500,
            temperature=0.7
        )

        answer = response.choices[0].message.content

        # ì„¸ì…˜ì— ëŒ€í™” ì €ì¥
        chat_sessions[filename].append({
            "role": "user",
            "content": request.question,
            "timestamp": time.time()
        })
        chat_sessions[filename].append({
            "role": "assistant",
            "content": answer,
            "timestamp": time.time()
        })

        return {
            "answer": answer,
            "tokens_used": response.usage.total_tokens,
            "session_id": filename,
            "message_count": len(chat_sessions[filename])
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

@router.get("/chat/history/{filename}")
async def get_chat_history(filename: str):
    """
    íŠ¹ì • íŒŒì¼ì˜ ì±„íŒ… ê¸°ë¡ ì¡°íšŒ
    """
    if filename not in chat_sessions:
        return {
            "history": [],
            "message_count": 0
        }

    return {
        "history": chat_sessions[filename],
        "message_count": len(chat_sessions[filename])
    }

@router.delete("/chat/history/{filename}")
async def clear_chat_history(filename: str):
    """
    íŠ¹ì • íŒŒì¼ì˜ ì±„íŒ… ê¸°ë¡ ì‚­ì œ
    """
    if filename in chat_sessions:
        del chat_sessions[filename]

    return {
        "message": "ì±„íŒ… ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
        "filename": filename
    }
